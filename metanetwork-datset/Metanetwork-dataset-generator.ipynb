{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b205eb07-e00f-447c-ac19-a1643abd199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef47ad3-2afc-42c2-baf9-2583705f1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class generates correlated memory vectors as decribed in (Benna, Fusi; 2021)\n",
    "class CorrelatedPatterns():\n",
    "    def __init__(self, \n",
    "                 L, #Length of each memory vector \n",
    "                 p, #Number of ancestors\n",
    "                 k, #Number of children per ancestor\n",
    "                 gamma): #Average overlap between child and ancestor. A value of one means each child is identical to its ancestor,\n",
    "                        #while a value of zero means each child is completely different from its ancestor.\n",
    "        self.L = L\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create three arrays to store the ancestor vectors, the descendant (child) vectors, and the difference vectors\n",
    "        self.ancestors = []\n",
    "        self.descendants = []\n",
    "        self.differences = []\n",
    "        \n",
    "        #For purposes of PyTorch dataset creation, we will create two new lists that do not themselves contain lists\n",
    "        self.descendants_singlelist = []\n",
    "        self.differences_singlelist = []\n",
    "        \n",
    "        for _ancestorIndex in range(p):\n",
    "            \n",
    "            #Each ancestor is initialized randomly\n",
    "            ancestor = np.random.choice((-1,1), size=(L))\n",
    "            self.ancestors.append(np.array(ancestor))\n",
    "            \n",
    "            self.descendants.append([])\n",
    "            #Initialize k descendants\n",
    "            for _descendantIndex in range(k):\n",
    "                descendant = torch.tensor([])\n",
    "                for __i in range(len(ancestor)):\n",
    "                    \n",
    "                    #With probability 1-gamma, the descendant memory is corrupted at this bit. \n",
    "                    if(random.uniform(0,1) < 1-gamma):\n",
    "                        descendant = torch.cat((descendant, (torch.tensor([ancestor[__i]]) * -1)))\n",
    "                    else: #Otherwise, the ancestor's memory at this bit is copied to the descendant.\n",
    "                        descendant = torch.cat((descendant, torch.tensor([ancestor[__i]])))\n",
    "                \n",
    "                #Save the memory\n",
    "                self.descendants[_ancestorIndex].append(descendant.clone().detach())\n",
    "                self.descendants_singlelist.append(descendant.clone().detach().reshape(1,-1))\n",
    "            \n",
    "            #Calculate the differences between the ancestor vectors and the child vectors\n",
    "            self.differences.append([])\n",
    "            for _descendantIndex in range(k):\n",
    "                self.differences[_ancestorIndex].append(torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex])\n",
    "                self.differences_singlelist.append((torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex]).reshape(1,-1))\n",
    "                \n",
    "        self.descendants_singlelist = torch.cat(self.descendants_singlelist)\n",
    "        self.differences_singlelist = torch.cat(self.differences_singlelist)\n",
    "\n",
    "#This subclass inherits the PyTorch Dataset class in order to create datasets of correlated memory.\n",
    "class SensoryData(Dataset):\n",
    "    def __init__(self, \n",
    "                 L,      #Length of each sample\n",
    "                 p,      #Number of parents\n",
    "                 k,      #Number of children per parent \n",
    "                 gamma   #Overlap between parent and children (1=identical, 0=no overlap)\n",
    "                ):\n",
    "        super().__init__()\n",
    "        c = CorrelatedPatterns(L, p, k, gamma)\n",
    "        memories = c.descendants_singlelist\n",
    "        \n",
    "        #Grab the memories generated by CorrelatedPatterns()\n",
    "        self.data = memories\n",
    "        self.x = memories\n",
    "        self.y = memories\n",
    "        self.n_samples = memories.shape[0]\n",
    "    \n",
    "    #Implement necessary helper functions\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9031082d-d8a0-4f57-881b-d95499c91b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_data_len10 = SensoryData(10, 60, 100, 0.9)\n",
    "torch.save(high_corr_data_len10, \"high_corr_dataset_len10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9dff13fd-b8ff-4625-b907-eb8716e7bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_corr_data_len10 = SensoryData(10, 60, 100, 0.6)\n",
    "torch.save(med_corr_data_len10, \"med_corr_dataset_len10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "18c097da-f784-4519-b2f7-2fbb739132fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr_data_len10 = SensoryData(10, 60, 100, 0.3)\n",
    "torch.save(low_corr_data_len10, \"low_corr_dataset_len10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "555b8285-e816-4d95-a629-f8a9af214f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_inputs, #Number of input units\n",
    "                 n_hiddens): #Number of hidden units\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eweight = nn.Parameter(torch.rand(n_hiddens, n_inputs), requires_grad=True)\n",
    "        #self.initial_weights = self.eweight.clone()\n",
    "        self.initial_state_dict = copy.deepcopy(self.state_dict())\n",
    "        \n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        \n",
    "    #Implement the forward pass\n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        \n",
    "        self.encoded = F.linear(X, self.eweight)\n",
    "        self.hidden_activations = torch.relu(self.encoded)\n",
    "        \n",
    "        self.decoded = F.linear(self.hidden_activations, self.eweight.T)\n",
    "        self.decoder_activations = torch.tanh(self.decoded)\n",
    "        return self.decoder_activations, self.hidden_activations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594d3255-a7a7-4346-aa8d-730a5d7f86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 10\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "    \n",
    "def update_focus_array():\n",
    "    pop_item = focus_array.pop(0)\n",
    "    push_item = next(iter(loader))\n",
    "    \n",
    "    focus_array.append(push_item)\n",
    "    \n",
    "    return push_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a3a890f-71a0-4212-ab13-75a483341de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function written by Huidi Li\n",
    "def compute_gradmask(model, grads, ratio=0.1): \n",
    "    masks = []\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        grads_shape = grads[i].shape\n",
    "        grads_sorted, grads_sort_idx = torch.sort(torch.abs(grads[i]).flatten())\n",
    "        min_idx = int(ratio * len(grads_sorted))\n",
    "        mask = abs(grads[i])<grads_sorted[min_idx]\n",
    "        masks.append(mask.reshape(grads_shape))\n",
    "    return masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f75204-4e72-4b56-af45-10cf47bd0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNoise(array):\n",
    "    \n",
    "    #mask_status = torch.rand(array.shape)\n",
    "    #mask_status = mask_status >= torch.sort(mask_status.flatten()).values[-1*int(arr.nelement()*(bit_mask_pct))]\n",
    "    #indices = np.random.permutation(np.arange(0,array.nelement(),1))\n",
    "    #array[indices[0]] = 0\n",
    "    #array[indices[1]] = 0\n",
    "    random_index = random.randint(0,len(array))\n",
    "    array[random_index % len(array)] = 0\n",
    "    array[(random_index + 1) % len(array)] = 0\n",
    "    \n",
    "    return array\n",
    "\n",
    "def train(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            outer_loop_epochs=1000,\n",
    "            inner_loop_epochs=100, \n",
    "            alpha=0.5,\n",
    "            reset_ratio=0.3\n",
    "        ):\n",
    "    \n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    model.train()\n",
    "    \n",
    "    initial_parameters = []\n",
    "    for param_ind, param in enumerate(model.parameters()):\n",
    "        initial_parameters.append(param)\n",
    "    \n",
    "    \n",
    "    weight_history = {}\n",
    "    \n",
    "    for outer_epoch in range(outer_loop_epochs):\n",
    "        \n",
    "        e_item = update_focus_array()\n",
    "        \n",
    "        Xs = torch.Tensor()\n",
    "        Ys = torch.Tensor()\n",
    "        for i in range(len(focus_array)):\n",
    "            Xs = torch.cat([Xs, focus_array[i][0]])\n",
    "            Ys = torch.cat([Ys, focus_array[i][1]])\n",
    "        \n",
    "        model_gradients_ma = []\n",
    "        \n",
    "        \n",
    "        W_old = []\n",
    "        for param_ind, param in enumerate(model.parameters()):\n",
    "            W_old.append(copy.deepcopy(param))\n",
    "        \n",
    "        \n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [inner_loop_epochs*0.25,\n",
    "                                                                      inner_loop_epochs*0.5],\n",
    "                                                                gamma=0.1,\n",
    "                                                                last_epoch=-1)\n",
    "        optimizer.param_groups[0]['lr'] = initial_lr\n",
    "        for inner_epoch in range(inner_loop_epochs):\n",
    "            for _ in range(Xs.shape[0]):\n",
    "                Xs[_] = AddNoise(Xs[_])\n",
    "            \n",
    "            predicted_y, net_hidden_activity = model(Xs)\n",
    "            loss = loss_function(predicted_y, Ys)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            for param_ind, param in enumerate(model.parameters()):\n",
    "                if(inner_epoch == 0):\n",
    "                    model_gradients_ma.append(torch.abs(param.grad))\n",
    "                else:\n",
    "                    model_gradients_ma[param_ind] = alpha*torch.abs(param.grad) + (1-alpha)*model_gradients_ma[param_ind]\n",
    "       \n",
    "        \n",
    "        W_new = []\n",
    "        for param_ind, param in enumerate(model.parameters()):\n",
    "            W_new.append(copy.deepcopy(param))\n",
    "        \n",
    "        #Save weights\n",
    "        weight_history[outer_epoch] = {'e_item': e_item, \n",
    "                               'W_old': W_old,\n",
    "                               'W_new': W_new, \n",
    "                               }\n",
    "        \n",
    "        #Reset weights\n",
    "        mask = compute_gradmask(model, model_gradients_ma, ratio=reset_ratio)\n",
    "        state_dict = model.state_dict()\n",
    "        param_index = 0\n",
    "        for param_name, param_value in state_dict.items():\n",
    "            state_dict[param_name][mask[param_index]] = model.initial_state_dict[param_name][mask[param_index]]\n",
    "            param_index += 1\n",
    "            \n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "    return weight_history      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fbf81ee-6b67-4699-8b50-96beaa6433ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 10\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0904695-a3f1-4c45-bd30-2dbe89d080c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30044ae3-3af7-4b51-ac83-d8c3822120cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "1a933322-465d-4475-8a81-16bd2b2a09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 30\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1a56a-48d9-4242-9165-1e209f6376a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3ddd0-cc88-4aeb-b567-769d1e0d3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03e29b-a29f-45d5-b477-a55a96d3909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 50\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e4efb-476b-449c-b601-a296c62adc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c3d60-976b-4d70-846c-d62c4ba3eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('7.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "6b5584fb-f595-4d6a-9a53-0123c02e71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4.pkl', 'rb') as f:\n",
    "    o_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "bfb3a777-08a0-4fbf-a0b0-217ffcb2d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(med_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 10\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "fa4edf69-72b0-41fd-9a02-365d8f8a4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "3cb8f0bc-9504-429f-9879-53b1c6097961",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90c18a-873d-4e05-8242-9db52783883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(med_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 30\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018903e-8b9f-47d5-b37c-1550c3c81395",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6ad0c-4e92-452d-b271-2fc58fe1c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('5.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec405d-db3a-45d9-884f-4a081fadf432",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(med_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 50\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdece87-4901-49af-96ff-79d9fd36bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea94f9-678c-437f-955d-53f513e7b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('8.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "4cae9045-ce7e-482a-90ba-672ccc17a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(low_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 10\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "be613eae-fac0-4487-a0fd-758479018796",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "8077f2d7-1f61-4a1d-bf53-a9fe9a225dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('3.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "48d8e9c6-2fa9-4290-b055-316362008945",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1 (2) (1).pkl', 'rb') as f:\n",
    "    output_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "92514fd9-c5d0-4b74-b183-a7e5b1f2acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel.eweight = output_data[5999]['W_new'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "eb0a677d-d52b-4d54-9212-5940559fb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_activation(array):\n",
    "    arr = array\n",
    "    arr[arr >= 0] = 1\n",
    "    arr[arr < 0] = -1\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "dd05c7fa-44ed-48d0-88c6-757765d911fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0\n",
    "for i in range(5999):\n",
    "    diff = step_activation(testmodel(output_data[i]['e_item'][0])[0]) - output_data[i]['e_item'][0]\n",
    "    avg += torch.count_nonzero(diff)/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "74e6bdbb-e5a7-47de-920a-1861c6660176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3003)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg/6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173a8a3-a78a-41ef-8be7-ab7d2b6d3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(low_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 30\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc6da7-19ca-4c1b-ba61-e457ca446973",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c31fc3-bf02-4426-92bc-95f066610468",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('6.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e315c-dfc0-48a6-90d0-46c9cae1d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(low_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 50\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45e60c-9cbd-4379-92e7-729989bcde6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=600,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904de81-395e-4886-8cea-5d770efb1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('9.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
