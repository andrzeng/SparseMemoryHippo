{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b205eb07-e00f-447c-ac19-a1643abd199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef47ad3-2afc-42c2-baf9-2583705f1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class generates correlated memory vectors as decribed in (Benna, Fusi; 2021)\n",
    "class CorrelatedPatterns():\n",
    "    def __init__(self, \n",
    "                 L, #Length of each memory vector \n",
    "                 p, #Number of ancestors\n",
    "                 k, #Number of children per ancestor\n",
    "                 gamma): #Average overlap between child and ancestor. A value of one means each child is identical to its ancestor,\n",
    "                        #while a value of zero means each child is completely different from its ancestor.\n",
    "        self.L = L\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create three arrays to store the ancestor vectors, the descendant (child) vectors, and the difference vectors\n",
    "        self.ancestors = []\n",
    "        self.descendants = []\n",
    "        self.differences = []\n",
    "        \n",
    "        #For purposes of PyTorch dataset creation, we will create two new lists that do not themselves contain lists\n",
    "        self.descendants_singlelist = []\n",
    "        self.differences_singlelist = []\n",
    "        \n",
    "        for _ancestorIndex in range(p):\n",
    "            \n",
    "            #Each ancestor is initialized randomly\n",
    "            ancestor = np.random.choice((-1,1), size=(L))\n",
    "            self.ancestors.append(np.array(ancestor))\n",
    "            \n",
    "            self.descendants.append([])\n",
    "            #Initialize k descendants\n",
    "            for _descendantIndex in range(k):\n",
    "                descendant = torch.tensor([])\n",
    "                for __i in range(len(ancestor)):\n",
    "                    \n",
    "                    #With probability 1-gamma, the descendant memory is corrupted at this bit. \n",
    "                    if(random.uniform(0,1) < 1-gamma):\n",
    "                        descendant = torch.cat((descendant, (torch.tensor([ancestor[__i]]) * -1)))\n",
    "                    else: #Otherwise, the ancestor's memory at this bit is copied to the descendant.\n",
    "                        descendant = torch.cat((descendant, torch.tensor([ancestor[__i]])))\n",
    "                \n",
    "                #Save the memory\n",
    "                self.descendants[_ancestorIndex].append(descendant.clone().detach())\n",
    "                self.descendants_singlelist.append(descendant.clone().detach().reshape(1,-1))\n",
    "            \n",
    "            #Calculate the differences between the ancestor vectors and the child vectors\n",
    "            self.differences.append([])\n",
    "            for _descendantIndex in range(k):\n",
    "                self.differences[_ancestorIndex].append(torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex])\n",
    "                self.differences_singlelist.append((torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex]).reshape(1,-1))\n",
    "                \n",
    "        self.descendants_singlelist = torch.cat(self.descendants_singlelist)\n",
    "        self.differences_singlelist = torch.cat(self.differences_singlelist)\n",
    "\n",
    "#This subclass inherits the PyTorch Dataset class in order to create datasets of correlated memory.\n",
    "class SensoryData(Dataset):\n",
    "    def __init__(self, \n",
    "                 L,      #Length of each sample\n",
    "                 p,      #Number of parents\n",
    "                 k,      #Number of children per parent \n",
    "                 gamma   #Overlap between parent and children (1=identical, 0=no overlap)\n",
    "                ):\n",
    "        super().__init__()\n",
    "        c = CorrelatedPatterns(L, p, k, gamma)\n",
    "        memories = c.descendants_singlelist\n",
    "        \n",
    "        #Grab the memories generated by CorrelatedPatterns()\n",
    "        self.data = memories\n",
    "        self.x = memories\n",
    "        self.y = memories\n",
    "        self.n_samples = memories.shape[0]\n",
    "    \n",
    "    #Implement necessary helper functions\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9031082d-d8a0-4f57-881b-d95499c91b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr_data2 = SensoryData(10, 100, 500, 0.5)\n",
    "torch.save(low_corr_data2, \"low_corr_dataset_len10.pt\")\n",
    "#low_corr_data2 = torch.load(\"low_corr_dataset2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e246613c-a575-4607-b12f-253083a3aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_data2 = SensoryData(10, 100, 500, 0.9)\n",
    "#torch.save(high_corr_data2, \"high_corr_dataset3.pt\")\n",
    "high_corr_data2 = torch.load(\"high_corr_dataset_len10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555b8285-e816-4d95-a629-f8a9af214f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_inputs, #Number of input units\n",
    "                 n_hiddens): #Number of hidden units\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eweight = nn.Parameter(torch.rand(n_hiddens, n_inputs), requires_grad=True)\n",
    "        #self.initial_weights = self.eweight.clone()\n",
    "        self.initial_state_dict = copy.deepcopy(self.state_dict())\n",
    "        \n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        \n",
    "    #Implement the forward pass\n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        \n",
    "        self.encoded = F.linear(X, self.eweight)\n",
    "        self.hidden_activations = F.relu(self.encoded)\n",
    "        self.decoded = F.linear(self.hidden_activations, self.eweight.T)\n",
    "        \n",
    "        return self.decoded, self.hidden_activations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965c7b0-f6f1-4890-8e27-4b9ca1da13f0",
   "metadata": {},
   "source": [
    "## High correlation training set (shuffled, ARRAY_LEN=10, correlation=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594d3255-a7a7-4346-aa8d-730a5d7f86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data2, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 10\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "    \n",
    "def update_focus_array():\n",
    "    pop_item = focus_array.pop(0)\n",
    "    push_item = next(iter(loader))\n",
    "    \n",
    "    focus_array.append(push_item)\n",
    "    \n",
    "    return push_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3a890f-71a0-4212-ab13-75a483341de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function written by Huidi Li\n",
    "def compute_gradmask(model, grads, ratio=0.1): \n",
    "    masks = []\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        grads_shape = grads[i].shape\n",
    "        grads_sorted, grads_sort_idx = torch.sort(torch.abs(grads[i]).flatten())\n",
    "        min_idx = int(ratio * len(grads_sorted))\n",
    "        mask = abs(grads[i])<grads_sorted[min_idx]\n",
    "        masks.append(mask.reshape(grads_shape))\n",
    "    return masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f75204-4e72-4b56-af45-10cf47bd0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNoise(array, bit_flip_chance):\n",
    "    toggle_status = torch.rand(array.shape) > bit_flip_chance\n",
    "    return array * toggle_status\n",
    "\n",
    "def train(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            outer_loop_epochs=1000,\n",
    "            inner_loop_epochs=100, \n",
    "            alpha=0.5,\n",
    "            reset_ratio=0.3\n",
    "        ):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    initial_parameters = []\n",
    "    for param_ind, param in enumerate(model.parameters()):\n",
    "        initial_parameters.append(param)\n",
    "    \n",
    "    \n",
    "    weight_history = {}\n",
    "    \n",
    "    for outer_epoch in range(outer_loop_epochs):\n",
    "        \n",
    "        e_item = update_focus_array()\n",
    "        \n",
    "        Xs = torch.Tensor()\n",
    "        Ys = torch.Tensor()\n",
    "        for i in range(len(focus_array)):\n",
    "            Xs = torch.cat([Xs, focus_array[i][0]])\n",
    "            Ys = torch.cat([Ys, focus_array[i][1]])\n",
    "        \n",
    "        model_gradients_ma = []\n",
    "        \n",
    "        \n",
    "        W_old = []\n",
    "        for param_ind, param in enumerate(model.parameters()):\n",
    "            W_old.append(copy.deepcopy(param))\n",
    "        \n",
    "        for inner_epoch in range(inner_loop_epochs):\n",
    "        \n",
    "            Xs = AddNoise(Xs, bit_flip_chance=0.2)\n",
    "        \n",
    "            predicted_y, net_hidden_activity = model(Xs)\n",
    "            loss = loss_function(predicted_y, Ys)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            for param_ind, param in enumerate(model.parameters()):\n",
    "                if(inner_epoch == 0):\n",
    "                    model_gradients_ma.append(torch.abs(param.grad))\n",
    "                else:\n",
    "                    model_gradients_ma[param_ind] = alpha*torch.abs(param.grad) + (1-alpha)*model_gradients_ma[param_ind]\n",
    "       \n",
    "        \n",
    "        W_new = []\n",
    "        for param_ind, param in enumerate(model.parameters()):\n",
    "            W_new.append(copy.deepcopy(param))\n",
    "        \n",
    "        #Save weights\n",
    "        weight_history[outer_epoch] = {'e_item': e_item, \n",
    "                               'W_old': W_old,\n",
    "                               'W_new': W_new, \n",
    "                               }\n",
    "        \n",
    "        #Reset weights\n",
    "        mask = compute_gradmask(model, model_gradients_ma, ratio=reset_ratio)\n",
    "        state_dict = model.state_dict()\n",
    "        param_index = 0\n",
    "        for param_name, param_value in state_dict.items():\n",
    "            state_dict[param_name][mask[param_index]] = model.initial_state_dict[param_name][mask[param_index]]\n",
    "            param_index += 1\n",
    "            \n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "    return weight_history\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56448a36-03ac-4357-8f7a-91a1129fca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(30,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1356e341-45a0-4c2c-aa2a-3c3298176001",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=50000,\n",
    "      inner_loop_epochs=32\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bb5c66f-72c5-47ca-b594-96243b155818",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('high_correlation_train.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d3422b2-2cab-4d7b-9801-7ca61bac3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data2, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93070988-7eaf-4ce6-95cb-4d953f2aa93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(30,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51f06ea0-abea-4981-9bdc-ac92f9192127",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=10000,\n",
    "      inner_loop_epochs=32\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b85c02f-ad36-42ec-aace-8e827a62a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('high_correlation_test.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea92caf-bbd6-4151-87b8-33c787621551",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(low_corr_data2, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c147b576-0452-4c4b-a3c5-7614157e845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(30,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd2956c-7bb5-4e2d-bf8f-8aef6a569f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=50000,\n",
    "      inner_loop_epochs=32\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78fca67f-9dd1-46b4-ab16-21e20db41dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('low_correlation_train.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26be1741-28fd-4554-817f-425c13d7f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(low_corr_data2, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e8f000-187e-4e9c-875f-d1a746719481",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(30,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534d64b4-fd54-4c65-a971-691e5e151129",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=10000,\n",
    "      inner_loop_epochs=32\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a64d24d-572f-4d3f-8576-de9ceeb503f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('low_correlation_test.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b25b9-7f04-4a95-b28c-deff828f66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4e1fe-cc0d-458e-b003-ba108390b791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c42e7-d92a-4102-b76c-430ff8bff7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
