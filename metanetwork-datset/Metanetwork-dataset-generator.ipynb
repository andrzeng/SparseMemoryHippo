{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b205eb07-e00f-447c-ac19-a1643abd199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef47ad3-2afc-42c2-baf9-2583705f1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class generates correlated memory vectors as decribed in (Benna, Fusi; 2021)\n",
    "class CorrelatedPatterns():\n",
    "    def __init__(self, \n",
    "                 L, #Length of each memory vector \n",
    "                 p, #Number of ancestors\n",
    "                 k, #Number of children per ancestor\n",
    "                 gamma): #Average overlap between child and ancestor. A value of one means each child is identical to its ancestor,\n",
    "                        #while a value of zero means each child is completely different from its ancestor.\n",
    "        self.L = L\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create three arrays to store the ancestor vectors, the descendant (child) vectors, and the difference vectors\n",
    "        self.ancestors = []\n",
    "        self.descendants = []\n",
    "        self.differences = []\n",
    "        \n",
    "        #For purposes of PyTorch dataset creation, we will create two new lists that do not themselves contain lists\n",
    "        self.descendants_singlelist = []\n",
    "        self.differences_singlelist = []\n",
    "        \n",
    "        for _ancestorIndex in range(p):\n",
    "            \n",
    "            #Each ancestor is initialized randomly\n",
    "            ancestor = np.random.choice((-1,1), size=(L))\n",
    "            self.ancestors.append(np.array(ancestor))\n",
    "            \n",
    "            self.descendants.append([])\n",
    "            #Initialize k descendants\n",
    "            for _descendantIndex in range(k):\n",
    "                descendant = torch.tensor([])\n",
    "                for __i in range(len(ancestor)):\n",
    "                    \n",
    "                    #With probability 1-gamma, the descendant memory is corrupted at this bit. \n",
    "                    if(random.uniform(0,1) < 1-gamma):\n",
    "                        descendant = torch.cat((descendant, (torch.tensor([ancestor[__i]]) * -1)))\n",
    "                    else: #Otherwise, the ancestor's memory at this bit is copied to the descendant.\n",
    "                        descendant = torch.cat((descendant, torch.tensor([ancestor[__i]])))\n",
    "                \n",
    "                #Save the memory\n",
    "                self.descendants[_ancestorIndex].append(descendant.clone().detach())\n",
    "                self.descendants_singlelist.append(descendant.clone().detach().reshape(1,-1))\n",
    "            \n",
    "            #Calculate the differences between the ancestor vectors and the child vectors\n",
    "            self.differences.append([])\n",
    "            for _descendantIndex in range(k):\n",
    "                self.differences[_ancestorIndex].append(torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex])\n",
    "                self.differences_singlelist.append((torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex]).reshape(1,-1))\n",
    "                \n",
    "        self.descendants_singlelist = torch.cat(self.descendants_singlelist)\n",
    "        self.differences_singlelist = torch.cat(self.differences_singlelist)\n",
    "\n",
    "#This subclass inherits the PyTorch Dataset class in order to create datasets of correlated memory.\n",
    "class SensoryData(Dataset):\n",
    "    def __init__(self, \n",
    "                 L,      #Length of each sample\n",
    "                 p,      #Number of parents\n",
    "                 k,      #Number of children per parent \n",
    "                 gamma   #Overlap between parent and children (1=identical, 0=no overlap)\n",
    "                ):\n",
    "        super().__init__()\n",
    "        c = CorrelatedPatterns(L, p, k, gamma)\n",
    "        memories = c.descendants_singlelist\n",
    "        \n",
    "        #Grab the memories generated by CorrelatedPatterns()\n",
    "        self.data = memories\n",
    "        self.x = memories\n",
    "        self.y = memories\n",
    "        self.n_samples = memories.shape[0]\n",
    "    \n",
    "    #Implement necessary helper functions\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9031082d-d8a0-4f57-881b-d95499c91b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_data_len10 = SensoryData(10, 10, 600, 0.9)\n",
    "torch.save(high_corr_data_len10, \"high_corr_dataset_len10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dff13fd-b8ff-4625-b907-eb8716e7bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_corr_data_len10 = SensoryData(10, 10, 600, 0.6)\n",
    "torch.save(med_corr_data_len10, \"med_corr_dataset_len10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c097da-f784-4519-b2f7-2fbb739132fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr_data_len10 = SensoryData(10, 10, 600, 0.3)\n",
    "torch.save(low_corr_data_len10, \"low_corr_dataset_len10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "555b8285-e816-4d95-a629-f8a9af214f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_inputs, #Number of input units\n",
    "                 n_hiddens): #Number of hidden units\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eweight = nn.Parameter(torch.rand(n_hiddens, n_inputs), requires_grad=True)\n",
    "        #self.initial_weights = self.eweight.clone()\n",
    "        self.initial_state_dict = copy.deepcopy(self.state_dict())\n",
    "        \n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        \n",
    "    #Implement the forward pass\n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        \n",
    "        self.encoded = F.linear(X, self.eweight)\n",
    "        self.hidden_activations = torch.relu(self.encoded)\n",
    "        \n",
    "        self.decoded = F.linear(self.hidden_activations, self.eweight.T)\n",
    "        self.decoder_activations = torch.tanh(self.decoded)\n",
    "        return self.decoder_activations, self.hidden_activations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594d3255-a7a7-4346-aa8d-730a5d7f86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 10\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "    \n",
    "def update_focus_array():\n",
    "    pop_item = focus_array.pop(0)\n",
    "    push_item = next(iter(loader))\n",
    "    \n",
    "    focus_array.append(push_item)\n",
    "    \n",
    "    return push_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a3a890f-71a0-4212-ab13-75a483341de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function written by Huidi Li\n",
    "def compute_gradmask(model, grads, ratio=0.1): \n",
    "    masks = []\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        grads_shape = grads[i].shape\n",
    "        grads_sorted, grads_sort_idx = torch.sort(torch.abs(grads[i]).flatten())\n",
    "        min_idx = int(ratio * len(grads_sorted))\n",
    "        mask = abs(grads[i])<grads_sorted[min_idx]\n",
    "        masks.append(mask.reshape(grads_shape))\n",
    "    return masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "43f75204-4e72-4b56-af45-10cf47bd0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddNoise(array):\n",
    "    \n",
    "    #mask_status = torch.rand(array.shape)\n",
    "    #mask_status = mask_status >= torch.sort(mask_status.flatten()).values[-1*int(arr.nelement()*(bit_mask_pct))]\n",
    "    #indices = np.random.permutation(np.arange(0,array.nelement(),1))\n",
    "    #array[indices[0]] = 0\n",
    "    #array[indices[1]] = 0\n",
    "    \n",
    "    arr = copy.deepcopy(array)\n",
    "    \n",
    "    random_index = random.randint(0,len(arr))\n",
    "    arr[random_index % len(arr)] = 0\n",
    "    arr[(random_index + 1) % len(arr)] = 0\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def train(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            outer_loop_epochs=1000,\n",
    "            inner_loop_epochs=100, \n",
    "            alpha=0.5,\n",
    "            reset_ratio=0.3\n",
    "    \n",
    "        ):\n",
    "    \n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    model.train()\n",
    "    \n",
    "    initial_parameters = []\n",
    "    for param_ind, param in enumerate(model.parameters()):\n",
    "        initial_parameters.append(param)\n",
    "    \n",
    "    \n",
    "    weight_history = {}\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for outer_epoch in range(outer_loop_epochs):\n",
    "        if(outer_epoch % 50 == 0):\n",
    "            print(\"Epoch \", outer_epoch)\n",
    "        e_item = update_focus_array()\n",
    "        \n",
    "        Xs = torch.Tensor()\n",
    "        Ys = torch.Tensor()\n",
    "        for i in range(len(focus_array)):\n",
    "            Xs = torch.cat([Xs, focus_array[i][0]])\n",
    "            Ys = torch.cat([Ys, focus_array[i][1]])\n",
    "        \n",
    "        model_gradients_ma = []\n",
    "        \n",
    "        \n",
    "        W_old = []\n",
    "        for param_ind, param in enumerate(model.parameters()):\n",
    "            W_old.append(copy.deepcopy(param))\n",
    "        \n",
    "        \n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [inner_loop_epochs*0.25,\n",
    "                                                               inner_loop_epochs*0.5,\n",
    "                                                               inner_loop_epochs*0.75],\n",
    "                                                                gamma=0.1,\n",
    "                                                                last_epoch=-1)\n",
    "        optimizer.param_groups[0]['lr'] = initial_lr\n",
    "        \n",
    "        \n",
    "        for inner_epoch in range(inner_loop_epochs):\n",
    "            trial_Xs = copy.deepcopy(Xs)\n",
    "            for _ in range(Xs.shape[0]):\n",
    "                trial_Xs[_] = AddNoise(trial_Xs[_])\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted_y, net_hidden_activity = model(trial_Xs)\n",
    "            \n",
    "            loss = loss_function(predicted_y, Ys)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            for param_ind, param in enumerate(model.parameters()):\n",
    "                if(inner_epoch == 0):\n",
    "                    model_gradients_ma.append(torch.abs(param.grad))\n",
    "                else:\n",
    "                    model_gradients_ma[param_ind] = alpha*torch.abs(param.grad) + (1-alpha)*model_gradients_ma[param_ind]\n",
    "        \n",
    "    \n",
    "        W_new = []\n",
    "        for param_ind, param in enumerate(model.parameters()):\n",
    "            W_new.append(copy.deepcopy(param))\n",
    "        \n",
    "        #Save weights\n",
    "        weight_history[outer_epoch] = {'e_item': e_item, \n",
    "                               'W_old': W_old,\n",
    "                               'W_new': W_new, \n",
    "                               }\n",
    "        \n",
    "        #Reset weights\n",
    "        mask = compute_gradmask(model, model_gradients_ma, ratio=reset_ratio)\n",
    "        state_dict = model.state_dict()\n",
    "        param_index = 0\n",
    "        for param_name, param_value in state_dict.items():\n",
    "            state_dict[param_name][mask[param_index]] = model.initial_state_dict[param_name][mask[param_index]]\n",
    "            param_index += 1\n",
    "            \n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "    return weight_history      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8fbf81ee-6b67-4699-8b50-96beaa6433ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 100\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e0904695-a3f1-4c45-bd30-2dbe89d080c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  50\n"
     ]
    }
   ],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=100,\n",
    "      inner_loop_epochs=256\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8eb4df56-fce9-4b9d-a9bd-bb425e9653a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_thresh(tensor):\n",
    "    tensor[tensor > 0] = 1\n",
    "    tensor[tensor <= 0] = -1\n",
    "    return tensor\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    successes = 0\n",
    "    trials = 0\n",
    "    for X, y in loader:\n",
    "        X[0] = AddNoise(X[0])\n",
    "        if(torch.count_nonzero(bin_thresh(model(X)[0]) - y) < 2):\n",
    "            successes += 1\n",
    "        trials += 1\n",
    "        \n",
    "    print(f'Accuracy rate: {successes}/{trials} = {successes/trials}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8f1f9e97-41de-4834-acea-fe5f29019369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate: 2725/6000 = 0.45416666666666666\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30044ae3-3af7-4b51-ac83-d8c3822120cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('high_corr_arraylen_100.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2949ed8d-38da-4d18-976e-38e5f6437286",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(med_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 30\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8301b8-05ee-4c15-ae21-77985653e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=400\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b285b6-e9b7-4cc0-adff-71ea9e77a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('5.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8098647-93b4-42be-835c-44323c59b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(low_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 30\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6143b367-0eba-46f0-9e21-8bcdf306be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c20dccbc-5f98-495f-a72e-0884da1d3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('6.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3b48f5-8efd-453d-baff-87241c0ad477",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(high_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 50\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6377d69d-a80e-4de2-addd-bbc15aa8cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "735cf908-6b78-4f64-9164-c176e9c3d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('7.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5596a820-7dfb-4163-868e-256fb3e1e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(med_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 50\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48f00b89-3fd7-434a-a521-3fbd5b7d49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "673413bf-0a25-4147-800b-f543f8ac8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('8.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd08c11-9a7c-4db3-82df-9140783796f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(low_corr_data_len10, batch_size=1, shuffle=True)\n",
    "\n",
    "focus_array = []\n",
    "ARRAY_LEN = 50\n",
    "for i in range(ARRAY_LEN):\n",
    "    focus_array.append(next(iter(loader)))\n",
    "\n",
    "\n",
    "model = Autoencoder(10,20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "337b6340-bb0b-4ef3-8ecc-8559bd16774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = train(model=model,\n",
    "      optimizer=optimizer,\n",
    "      loss_function=loss_function,\n",
    "      outer_loop_epochs=6000,\n",
    "      inner_loop_epochs=64\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abf3fcfd-8f90-40b1-93d8-080093d884ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('9.pkl', 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fc4f5-7524-4700-af6a-6b07abbf41a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
