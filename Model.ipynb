{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e6a8461-1a41-4799-b495-e5e22fb95a5e",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b9014a-eacb-4a10-bbc0-ec665ecef187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9851dd8-d6d8-4b46-8335-ed27172752e5",
   "metadata": {},
   "source": [
    "# Implement training dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d84f3ff-31a2-4525-a932-d8136b027a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class generates correlated memory vectors as decribed in (Benna, Fusi; 2021)\n",
    "class CorrelatedPatterns():\n",
    "    def __init__(self, \n",
    "                 L, #Length of each memory vector \n",
    "                 p, #Number of ancestors\n",
    "                 k, #Number of children per ancestor\n",
    "                 gamma): #Average overlap between child and ancestor. A value of one means each child is identical to its ancestor,\n",
    "                        #while a value of zero means each child is completely different from its ancestor.\n",
    "        self.L = L\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create three arrays to store the ancestor vectors, the descendant (child) vectors, and the difference vectors\n",
    "        self.ancestors = []\n",
    "        self.descendants = []\n",
    "        self.differences = []\n",
    "        \n",
    "        #For purposes of PyTorch dataset creation, we will create two new lists that do not themselves contain lists\n",
    "        self.descendants_singlelist = []\n",
    "        self.differences_singlelist = []\n",
    "        \n",
    "        for _ancestorIndex in range(p):\n",
    "            \n",
    "            #Each ancestor is initialized randomly\n",
    "            ancestor = np.random.choice((0,1), size=(L))\n",
    "            self.ancestors.append(np.array(ancestor))\n",
    "            \n",
    "            self.descendants.append([])\n",
    "            #Initialize k descendants\n",
    "            for _descendantIndex in range(k):\n",
    "                descendant = torch.tensor([])\n",
    "                for __i in range(len(ancestor)):\n",
    "                    \n",
    "                    #With probability 1-gamma, the descendant memory is corrupted at this bit. \n",
    "                    if(random.uniform(0,1) < 1-gamma):\n",
    "                        descendant = torch.cat((descendant, torch.tensor([0 if random.uniform(0,1) < 0.5 else 1])))\n",
    "                    else: #Otherwise, the ancestor's memory at this bit is copied to the descendant.\n",
    "                        descendant = torch.cat((descendant, torch.tensor([ancestor[__i]])))\n",
    "                \n",
    "                #Save the memory\n",
    "                self.descendants[_ancestorIndex].append(descendant.clone().detach())\n",
    "                self.descendants_singlelist.append(descendant.clone().detach().reshape(1,-1))\n",
    "            \n",
    "            #Calculate the differences between the ancestor vectors and the child vectors\n",
    "            self.differences.append([])\n",
    "            for _descendantIndex in range(k):\n",
    "                self.differences[_ancestorIndex].append(torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex])\n",
    "                self.differences_singlelist.append((torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex]).reshape(1,-1))\n",
    "                \n",
    "        self.descendants_singlelist = torch.cat(self.descendants_singlelist)\n",
    "        self.differences_singlelist = torch.cat(self.differences_singlelist)\n",
    "\n",
    "#This subclass inherits the PyTorch Dataset class in order to create datasets of correlated memory.\n",
    "class SensoryData(Dataset):\n",
    "    def __init__(self, \n",
    "                 L,      #Length of each sample\n",
    "                 p,      #Number of parents\n",
    "                 k,      #Number of children per parent \n",
    "                 gamma   #Overlap between parent and children (1=identical, 0=no overlap)\n",
    "                ):\n",
    "        super().__init__()\n",
    "        c = CorrelatedPatterns(L, p, k, gamma)\n",
    "        memories = c.descendants_singlelist\n",
    "        \n",
    "        #Grab the memories generated by CorrelatedPatterns()\n",
    "        self.data = memories\n",
    "        self.x = memories\n",
    "        self.y = memories\n",
    "        self.n_samples = memories.shape[0]\n",
    "    \n",
    "    #Implement necessary helper functions\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7a792-8b5d-486e-ab6f-71f4bfcf07a3",
   "metadata": {},
   "source": [
    "# Write models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f9b38ad-0f3f-40b0-a67c-6aaed0656e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement autoencoder model\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_inputs, #Number of input units\n",
    "                 n_hiddens): #Number of hidden units\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        \n",
    "        #The encoder and decoder will be linear layers\n",
    "        self.encoder = nn.Linear(n_inputs, n_hiddens)\n",
    "        self.decoder = nn.Linear(n_hiddens, n_inputs)\n",
    "        \n",
    "    #Implement the forward pass\n",
    "    def forward(self, X):\n",
    "        self.encoded = F.relu(self.encoder(X))\n",
    "        self.decoded = self.decoder(self.encoded)\n",
    "        \n",
    "        return self.decoded\n",
    "    \n",
    "#Implement the input encoder model\n",
    "#This is an autoencoder without the hidden layer. This model cannot just learn the identity function, because noise \n",
    "#will be applied to input data during training.\n",
    "class InputEncoder(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.encoder = nn.Linear(n_inputs, n_inputs)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.encoder.state_dict()['weight'].diagonal(0).zero_()\n",
    "        self.encoded = self.encoder(X)\n",
    "        \n",
    "        return self.encoded\n",
    "    \n",
    "    \n",
    "class RandomEncoder(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Parameter(torch.rand(n_inputs, n_inputs), requires_grad=False)\n",
    "        self.decoder = nn.Parameter(torch.rand(n_inputs, n_inputs), requires_grad=False)\n",
    "        \n",
    "        self.encoder_thresholds = nn.Parameter(torch.rand(n_inputs), requires_grad=True)\n",
    "        self.decoder_thresholds = nn.Parameter(torch.rand(n_inputs), requires_grad=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.encoded = F.linear(X, self.encoder)\n",
    "        self.encoded = torch.where(self.encoded > self.encoder_thresholds, self.encoded, 0)\n",
    "        self.decoded = F.linear(self.encoded, self.decoder)\n",
    "        self.decoded = torch.where(self.decoded > self.decoder_thresholds, self.decoded, 0)\n",
    "        \n",
    "        return self.decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fdb468b-253a-4a54-ac7e-9175001adad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.state_dict()['weight'][0][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "581224dc-31d2-44c3-b7d8-5502ce5a93ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.state_dict()['weight'].diagonal(0).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cc8fbd8-9de3-4e0e-a276-201267be2c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.diagonal>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.state_dict()['weight'].diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ada7222-2932-4a9a-b41d-f8e7ac26f11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1057, -0.3100, -0.0562, -0.0264, -0.1247,  0.1058, -0.2543, -0.1946,\n",
       "        -0.0225, -0.0884], grad_fn=<DiagonalBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.weight.diagonal(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6026634c-0fd7-4113-bced-6655c00a7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "re = RandomEncoder(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e5c08b8-614a-4e80-8810-3a35f23a4eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.2120, 11.5292, 13.9911, 16.3974, 11.2524, 13.2661, 13.8382, 11.8183,\n",
       "        13.8069, 11.7480])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(10)\n",
    "re(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f64838-70bd-4eb1-95c4-5828cf5cbb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get best device if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc43c2-f964-4ba4-a183-c3ede0eae6b6",
   "metadata": {},
   "source": [
    "# Implement training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58fc7026-9e1e-42d8-8b28-88959b9bf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, #Dataloader\n",
    "          model,  #Model to be trained\n",
    "          loss_function, #Loss function\n",
    "          optimizer, #Optimizer\n",
    "          enforce_sparsity=True, #Whether to add a sparsity penalty or not (valid only for the autoencoder model) \n",
    "          desired_sparsity=0.05, #Desired sparsity level\n",
    "          n_epochs=100 #number of epochs\n",
    "         ):\n",
    "    \n",
    "    #Toggle to training mode\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        #Iterate through the DataLoader's batches\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            #Get the model's prediction of the input\n",
    "            predicted_y = model(X)\n",
    "            \n",
    "            #Calculate the loss\n",
    "            loss = loss_function(predicted_y, y)\n",
    "            \n",
    "            #If the enforce_sparsity flag is True, this code runs.\n",
    "            if(enforce_sparsity == True):\n",
    "                \n",
    "                #We will calculate the average hidden activity over every training example.\n",
    "                total_hidden_activations = torch.zeros(loader.batch_size, model.n_hiddens)\n",
    "                \n",
    "                #Loop through every training example\n",
    "                for _, (_X, _y) in enumerate(loader):\n",
    "                    _X, _y = _X.to(device), _y.to(device)\n",
    "                    \n",
    "                    #Accumulate the hidden activity generated by the training example\n",
    "                    total_hidden_activations += F.relu(model.encoder(_X))\n",
    "                    \n",
    "                #Take the average\n",
    "                total_hidden_activations /= len(loader)\n",
    "                \n",
    "                #Calculate the sparsity penalty using Kullback-Leibler divergence. The motivation behind this algorithm is in a paper linked on the Github page.\n",
    "                sparsity_penalty = torch.abs(torch.nn.functional.kl_div(total_hidden_activations, torch.Tensor([desired_sparsity]).repeat(total_hidden_activations.shape)))\n",
    "                \n",
    "                #Add the sparsity penalty to the overall loss\n",
    "                loss += sparsity_penalty\n",
    "            \n",
    "            #Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            #Update the optimizer\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dc4f2-19df-4052-a7f7-ba53dbee3730",
   "metadata": {},
   "source": [
    "# Implement helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156ce0de-4285-47e2-bfac-0e5f33f170a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add noise to a memory vector\n",
    "def mutate(data, #An array of memories\n",
    "           gamma #The level of overlap between the noised vectors and the old vectors. A value of 0 results in the most noise.\n",
    "          ):\n",
    "    \n",
    "    #Create a new array of memory vectors in the shape of the old array\n",
    "    newdata = torch.zeros_like(data)\n",
    "    \n",
    "    #Iterate through the memories\n",
    "    for row in range(data.shape[0]):     #Each row is a memory\n",
    "        for col in range(data.shape[1]): #Each column is an entry in that memory vector\n",
    "            \n",
    "            #Add noise based on the gamma parameter\n",
    "            if random.uniform(0,1) > gamma:\n",
    "                newdata[row][col] = 0 if random.uniform(0,1) < 0.5 else 1\n",
    "            else:\n",
    "                newdata[row][col] = data[row][col]\n",
    "    return newdata\n",
    "\n",
    "\n",
    "#Test how many memories can be reconstructed after training\n",
    "def evaluate(loader,  #dataloader of trained data\n",
    "             model, #model\n",
    "             input_length): #memory length \n",
    "    \n",
    "    #Switch to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    #Count the number of recalled memories\n",
    "    correctly_recalled_memories = 0\n",
    "    \n",
    "    #Turn off gradient calculations for speed\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #Iterate through each training example\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #Add noise to the input memory\n",
    "            X = mutate(X, 0.8) \n",
    "            \n",
    "            #Get prediction\n",
    "            predicted_y = model(X) \n",
    "            \n",
    "            #Calculate absolute error\n",
    "            error = torch.abs(y - predicted_y)\n",
    "            \n",
    "            #Does the reconstructed memory sufficiently match the original memory?\n",
    "            matching_count = torch.count_nonzero(error < 0.5)\n",
    "            if(matching_count/input_length >= 0.9): #If so, increment the count of successfully reconstructed memories.\n",
    "                correctly_recalled_memories += 1\n",
    "            \n",
    "    return correctly_recalled_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85b575-d82a-4fb9-8722-83d0a94e8e77",
   "metadata": {},
   "source": [
    "# the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4d22e4-c1e0-4ebe-929e-23e7eaff8700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse autoencoder performance: Given 20 memory vectors, 10 were recalled.\n",
      "Input encoder performance: Given 20 memory vectors, 13 were recalled.\n",
      "Sparse autoencoder performance: Given 40 memory vectors, 24 were recalled.\n",
      "Input encoder performance: Given 40 memory vectors, 10 were recalled.\n",
      "Sparse autoencoder performance: Given 60 memory vectors, 40 were recalled.\n",
      "Input encoder performance: Given 60 memory vectors, 43 were recalled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16664/415499496.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m#Train the models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_autoencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sparsity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_sparsity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.075\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sparsity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16664/3679796715.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loader, model, loss_function, optimizer, enforce_sparsity, desired_sparsity, n_epochs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;31m#Calculate the sparsity penalty using Kullback-Leibler divergence. The motivation behind this algorithm is in a paper linked on the Github page.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0msparsity_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_hidden_activations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdesired_sparsity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_hidden_activations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;31m#Add the sparsity penalty to the overall loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mkl_div\u001b[1;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[0;32m   2914\u001b[0m             \u001b[0mreduction_enum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2916\u001b[1;33m     \u001b[0mreduced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_target\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2918\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batchmean\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory_len = 3 #Set the length of each memory vector to 300, as in the paper.\n",
    "\n",
    "#Record the reconstruction statistics for the sparse autoencoder and input encoder\n",
    "sparse_autoencoder_results = [] \n",
    "input_encoder_results = []\n",
    "\n",
    "\n",
    "k = 20 #Fix the number of children per parent \n",
    "\n",
    "#Loop over the number of parent memories to create\n",
    "for experimentNum in range(25):\n",
    "    \n",
    "    #Generate a dataset of correlated memories\n",
    "    data = SensoryData(memory_len, \n",
    "                       experimentNum+1, #Number of parents\n",
    "                       k, #Number of children per parent\n",
    "                       0.6) #Each child memory will, on average, have a 60% overlap with its parent memory\n",
    "    dataloader = DataLoader(dataset=data, batch_size=5, shuffle=True)     #Initialize the training dataloader\n",
    "    eval_dataloader = DataLoader(dataset=data, batch_size=1, shuffle=True) #Initialize the evaluation dataloader. We want to evaulate each memory by itself, thus batch_size=1\n",
    "    \n",
    "    #Initialize models\n",
    "    sparse_autoencoder = Autoencoder(memory_len, 2*memory_len).to(device)\n",
    "    input_encoder = InputEncoder(memory_len).to(device)\n",
    "    \n",
    "    #Initialize loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "                           \n",
    "    #Initialize optimizers\n",
    "    sparse_optimizer = torch.optim.Adam(sparse_autoencoder.parameters(), lr=1e-3)\n",
    "    input_optimizer = torch.optim.Adam(input_encoder.parameters(), lr=1e-3)\n",
    "\n",
    "    #Train the models                      \n",
    "    train(dataloader, sparse_autoencoder, loss_function, sparse_optimizer, enforce_sparsity=True, desired_sparsity=0.075)\n",
    "    train(dataloader, input_encoder, loss_function, input_optimizer, enforce_sparsity=False)\n",
    "\n",
    "    #Validate the models and record the results\n",
    "    sparse_autoencoder_recalled = evaluate(eval_dataloader, sparse_autoencoder, memory_len)\n",
    "    input_encoder_recalled = evaluate(eval_dataloader, input_encoder, memory_len)\n",
    "    \n",
    "    #Append results      \n",
    "    sparse_autoencoder_results.append((k*(experimentNum+1), sparse_autoencoder_recalled))\n",
    "    input_encoder_results.append((k*(experimentNum+1), input_encoder_recalled))\n",
    "\n",
    "    print(f\"Sparse autoencoder performance: Given {k*(experimentNum+1)} memory vectors, {sparse_autoencoder_recalled} were recalled.\")\n",
    "    print(f\"Input encoder performance: Given {k*(experimentNum+1)} memory vectors, {input_encoder_recalled} were recalled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea03c2-5a6c-4b7b-b794-753166b3dd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
