{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement training dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class generates correlated memory vectors as decribed in (Benna, Fusi; 2021)\n",
    "class CorrelatedPatterns():\n",
    "    def __init__(self, \n",
    "                 L, #Length of each memory vector \n",
    "                 p, #Number of ancestors\n",
    "                 k, #Number of children per ancestor\n",
    "                 gamma): #Average overlap between child and ancestor. A value of one means each child is identical to its ancestor,\n",
    "                        #while a value of zero means each child is completely different from its ancestor.\n",
    "        self.L = L\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create three arrays to store the ancestor vectors, the descendant (child) vectors, and the difference vectors\n",
    "        self.ancestors = []\n",
    "        self.descendants = []\n",
    "        self.differences = []\n",
    "        \n",
    "        #For purposes of PyTorch dataset creation, we will create two new lists that do not themselves contain lists\n",
    "        self.descendants_singlelist = []\n",
    "        self.differences_singlelist = []\n",
    "        \n",
    "        for _ancestorIndex in range(p):\n",
    "            \n",
    "            #Each ancestor is initialized randomly\n",
    "            ancestor = np.random.choice((0,1), size=(L))\n",
    "            self.ancestors.append(np.array(ancestor))\n",
    "            \n",
    "            self.descendants.append([])\n",
    "            #Initialize k descendants\n",
    "            for _descendantIndex in range(k):\n",
    "                descendant = torch.tensor([])\n",
    "                for __i in range(len(ancestor)):\n",
    "                    \n",
    "                    #With probability 1-gamma, the descendant memory is corrupted at this bit. \n",
    "                    if(random.uniform(0,1) < 1-gamma):\n",
    "                        descendant = torch.cat((descendant, torch.tensor([0 if random.uniform(0,1) < 0.5 else 1])))\n",
    "                    else: #Otherwise, the ancestor's memory at this bit is copied to the descendant.\n",
    "                        descendant = torch.cat((descendant, torch.tensor([ancestor[__i]])))\n",
    "                \n",
    "                #Save the memory\n",
    "                self.descendants[_ancestorIndex].append(descendant.clone().detach())\n",
    "                self.descendants_singlelist.append(descendant.clone().detach().reshape(1,-1))\n",
    "            \n",
    "            #Calculate the differences between the ancestor vectors and the child vectors\n",
    "            self.differences.append([])\n",
    "            for _descendantIndex in range(k):\n",
    "                self.differences[_ancestorIndex].append(torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex])\n",
    "                self.differences_singlelist.append((torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex]).reshape(1,-1))\n",
    "                \n",
    "        self.descendants_singlelist = torch.cat(self.descendants_singlelist)\n",
    "        self.differences_singlelist = torch.cat(self.differences_singlelist)\n",
    "\n",
    "#This subclass inherits the PyTorch Dataset class in order to create datasets of correlated memory.\n",
    "class SensoryData(Dataset):\n",
    "    def __init__(self, \n",
    "                 L,      #Length of each sample\n",
    "                 p,      #Number of parents\n",
    "                 k,      #Number of children per parent \n",
    "                 gamma   #Overlap between parent and children (1=identical, 0=no overlap)\n",
    "                ):\n",
    "        super().__init__()\n",
    "        c = CorrelatedPatterns(L, p, k, gamma)\n",
    "        memories = c.descendants_singlelist\n",
    "        \n",
    "        #Grab the memories generated by CorrelatedPatterns()\n",
    "        self.data = memories\n",
    "        self.x = memories\n",
    "        self.y = memories\n",
    "        self.n_samples = memories.shape[0]\n",
    "    \n",
    "    #Implement necessary helper functions\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement autoencoder model\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_inputs, #Number of input units\n",
    "                 n_hiddens): #Number of hidden units\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        \n",
    "        #The encoder and decoder will be linear layers\n",
    "        self.encoder = nn.Linear(n_inputs, n_hiddens)\n",
    "        self.decoder = nn.Linear(n_hiddens, n_inputs)\n",
    "        \n",
    "    #Implement the forward pass\n",
    "    def forward(self, X):\n",
    "        self.encoded = F.relu(self.encoder(X))\n",
    "        self.decoded = self.decoder(self.encoded)\n",
    "        \n",
    "        return self.decoded\n",
    "    \n",
    "#Implement the input encoder model\n",
    "#This is an autoencoder without the hidden layer. This model cannot just learn the identity function, because noise \n",
    "#will be applied to input data during training.\n",
    "class InputEncoder(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.encoder = nn.Linear(n_inputs, n_inputs)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.encoder.state_dict()['weight'].diagonal(0).zero_()\n",
    "        self.encoded = self.encoder(X)\n",
    "        \n",
    "        return self.encoded\n",
    "    \n",
    "    \n",
    "class RandomEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_inputs, #memory length\n",
    "                 encoding_sparsity=0.05): #desired hidden layer sparsity \n",
    "        super().__init__()\n",
    "        self.encoder = nn.Parameter(torch.rand(n_inputs, n_inputs), requires_grad=False)\n",
    "        self.decoder = nn.Linear(n_inputs, n_inputs)\n",
    "        self.encoding_sparsity = encoding_sparsity\n",
    "        self.n_inputs = n_inputs\n",
    "        \n",
    "    def forward(self, X):\n",
    "#        X = X.reshape(-1,self.n_inputs)\n",
    "        self.encoded = F.linear(X, self.encoder)\n",
    "        \n",
    "        #Get the sorted indices in ascending order\n",
    "        \n",
    "        \n",
    "        #zero out the lowest activations, leaving only the top encoding_sparsity*n_inputs activations intact\n",
    "        for batch in range(len(X)):\n",
    "            ascending = torch.sort(self.encoded[batch]).indices\n",
    "            for i in range(math.floor(len(ascending)*(1-self.encoding_sparsity))): \n",
    "                self.encoded[batch][ascending[i]] = 0\n",
    "            for i in range(len(ascending)):\n",
    "                if(self.encoded[batch][ascending[i]] != 0):\n",
    "                    self.encoded[batch][ascending[i]] = 1\n",
    "            \n",
    "        self.decoded = self.decoder(self.encoded)\n",
    "        \n",
    "        return self.decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get best device if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, #Dataloader\n",
    "          model,  #Model to be trained\n",
    "          loss_function, #Loss function\n",
    "          optimizer, #Optimizer\n",
    "          enforce_sparsity=True, #Whether to add a sparsity penalty or not (valid only for the autoencoder model) \n",
    "          desired_sparsity=0.05, #Desired sparsity level\n",
    "          n_epochs=100 #number of epochs\n",
    "         ):\n",
    "    \n",
    "    #Toggle to training mode\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        #Iterate through the DataLoader's batches\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            #Get the model's prediction of the input\n",
    "            predicted_y = model(X)\n",
    "            \n",
    "            #Calculate the loss\n",
    "            loss = loss_function(predicted_y, y)\n",
    "            \n",
    "            #If the enforce_sparsity flag is True, this code runs.\n",
    "            if(enforce_sparsity == True):\n",
    "                \n",
    "                #We will calculate the average hidden activity over every training example.\n",
    "                total_hidden_activations = torch.zeros(loader.batch_size, model.n_hiddens)\n",
    "                \n",
    "                #Loop through every training example\n",
    "                for _, (_X, _y) in enumerate(loader):\n",
    "                    _X, _y = _X.to(device), _y.to(device)\n",
    "                    \n",
    "                    #Accumulate the hidden activity generated by the training example\n",
    "                    total_hidden_activations += F.relu(model.encoder(_X))\n",
    "                    \n",
    "                #Take the average\n",
    "                total_hidden_activations /= len(loader)\n",
    "                \n",
    "                #Calculate the sparsity penalty using Kullback-Leibler divergence. The motivation behind this algorithm is in a paper linked on the Github page.\n",
    "                sparsity_penalty = torch.abs(torch.nn.functional.kl_div(total_hidden_activations, torch.Tensor([desired_sparsity]).repeat(total_hidden_activations.shape)))\n",
    "                \n",
    "                #Add the sparsity penalty to the overall loss\n",
    "                loss += sparsity_penalty\n",
    "            \n",
    "            #Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            #Update the optimizer\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add noise to a memory vector\n",
    "def mutate(data, #An array of memories\n",
    "           gamma #The level of overlap between the noised vectors and the old vectors. A value of 0 results in the most noise.\n",
    "          ):\n",
    "    \n",
    "    #Create a new array of memory vectors in the shape of the old array\n",
    "    newdata = torch.zeros_like(data)\n",
    "    \n",
    "    #Iterate through the memories\n",
    "    for row in range(data.shape[0]):     #Each row is a memory\n",
    "        for col in range(data.shape[1]): #Each column is an entry in that memory vector\n",
    "            \n",
    "            #Add noise based on the gamma parameter\n",
    "            if random.uniform(0,1) > gamma:\n",
    "                newdata[row][col] = 0 if random.uniform(0,1) < 0.5 else 1\n",
    "            else:\n",
    "                newdata[row][col] = data[row][col]\n",
    "    return newdata\n",
    "\n",
    "\n",
    "#Test how many memories can be reconstructed after training\n",
    "def evaluate(loader,  #dataloader of trained data\n",
    "             model, #model\n",
    "             input_length): #memory length \n",
    "    \n",
    "    #Switch to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    #Count the number of recalled memories\n",
    "    correctly_recalled_memories = 0\n",
    "    \n",
    "    #Turn off gradient calculations for speed\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #Iterate through each training example\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #Add noise to the input memory\n",
    "            X = mutate(X, 0.8) \n",
    "            \n",
    "            #Get prediction\n",
    "            predicted_y = model(X) \n",
    "            \n",
    "            #Calculate absolute error\n",
    "            error = torch.abs(y - predicted_y)\n",
    "            \n",
    "            #Does the reconstructed memory sufficiently match the original memory?\n",
    "            matching_count = torch.count_nonzero(error < 0.5)\n",
    "            if(matching_count/input_length >= 0.9): #If so, increment the count of successfully reconstructed memories.\n",
    "                correctly_recalled_memories += 1\n",
    "            \n",
    "    return correctly_recalled_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse autoencoder performance: Given 20 memory vectors, 20 were recalled.\n",
      "Input encoder performance: Given 20 memory vectors, 20 were recalled.\n",
      "Random encoder performance: Given 20 memory vectors, 12 were recalled.\n",
      "\n",
      "Sparse autoencoder performance: Given 40 memory vectors, 40 were recalled.\n",
      "Input encoder performance: Given 40 memory vectors, 40 were recalled.\n",
      "Random encoder performance: Given 40 memory vectors, 9 were recalled.\n",
      "\n",
      "Sparse autoencoder performance: Given 60 memory vectors, 59 were recalled.\n",
      "Input encoder performance: Given 60 memory vectors, 60 were recalled.\n",
      "Random encoder performance: Given 60 memory vectors, 12 were recalled.\n",
      "\n",
      "Sparse autoencoder performance: Given 80 memory vectors, 79 were recalled.\n",
      "Input encoder performance: Given 80 memory vectors, 78 were recalled.\n",
      "Random encoder performance: Given 80 memory vectors, 7 were recalled.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "memory_len = 300 #Set the length of each memory vector to 300, as in the paper.\n",
    "\n",
    "#Record the reconstruction statistics for the sparse autoencoder and input encoder\n",
    "sparse_autoencoder_results = [] \n",
    "input_encoder_results = []\n",
    "random_encoder_results = []\n",
    "\n",
    "k = 20 #Fix the number of children per parent \n",
    "\n",
    "#Loop over the number of parent memories to create\n",
    "for experimentNum in range(25):\n",
    "    \n",
    "    #Generate a dataset of correlated memories\n",
    "    data = SensoryData(memory_len, \n",
    "                       experimentNum+1, #Number of parents\n",
    "                       k, #Number of children per parent\n",
    "                       0.6) #Each child memory will, on average, have a 60% overlap with its parent memory\n",
    "    dataloader = DataLoader(dataset=data, batch_size=5, shuffle=True)     #Initialize the training dataloader\n",
    "    eval_dataloader = DataLoader(dataset=data, batch_size=1, shuffle=True) #Initialize the evaluation dataloader. We want to evaulate each memory by itself, thus batch_size=1\n",
    "    \n",
    "    #Initialize models\n",
    "    sparse_autoencoder = Autoencoder(memory_len, 2*memory_len).to(device)\n",
    "    input_encoder = InputEncoder(memory_len).to(device)\n",
    "    random_encoder = RandomEncoder(memory_len).to(device)\n",
    "    \n",
    "    #Initialize loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "                           \n",
    "    #Initialize optimizers\n",
    "    sparse_optimizer = torch.optim.Adam(sparse_autoencoder.parameters(), lr=1e-3)\n",
    "    input_optimizer = torch.optim.Adam(input_encoder.parameters(), lr=1e-3)\n",
    "    random_optimizer = torch.optim.Adam(random_encoder.parameters(), lr=1e-3)\n",
    "    \n",
    "    #Train the models                      \n",
    "    train(dataloader, sparse_autoencoder, loss_function, sparse_optimizer, enforce_sparsity=True, desired_sparsity=0.075)\n",
    "    train(dataloader, input_encoder, loss_function, input_optimizer, enforce_sparsity=False)\n",
    "    train(dataloader, random_encoder, loss_function, random_optimizer, enforce_sparsity=False)\n",
    "    \n",
    "    #Validate the models and record the results\n",
    "    sparse_autoencoder_recalled = evaluate(eval_dataloader, sparse_autoencoder, memory_len)\n",
    "    input_encoder_recalled = evaluate(eval_dataloader, input_encoder, memory_len)\n",
    "    random_encoder_recalled = evaluate(eval_dataloader, random_encoder, memory_len)\n",
    "    \n",
    "    #Append results      \n",
    "    sparse_autoencoder_results.append((k*(experimentNum+1), sparse_autoencoder_recalled))\n",
    "    input_encoder_results.append((k*(experimentNum+1), input_encoder_recalled))\n",
    "    random_encoder_results.append((k*(experimentNum+1), random_encoder_recalled))\n",
    "    \n",
    "    print(f\"Sparse autoencoder performance: Given {k*(experimentNum+1)} memory vectors, {sparse_autoencoder_recalled} were recalled.\")\n",
    "    print(f\"Input encoder performance: Given {k*(experimentNum+1)} memory vectors, {input_encoder_recalled} were recalled.\")\n",
    "    print(f\"Random encoder performance: Given {k*(experimentNum+1)} memory vectors, {random_encoder_recalled} were recalled.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
